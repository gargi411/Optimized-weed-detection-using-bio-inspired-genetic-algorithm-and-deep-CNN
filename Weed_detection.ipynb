{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmjZNBx0vb71KBXozcqGqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gargi411/Optimized-weed-detection-using-bio-inspired-genetic-algorithm-and-deep-CNN/blob/main/Weed_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLUlbXxYowd1",
        "outputId": "505c5206-c309-4b73-bdcd-23b6c5aadf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: install + imports\n",
        "!pip -q install tensorflow-datasets\n",
        "\n",
        "import os, math, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "print(\"TF:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: load dataset (will download ~5 GB)\n",
        "ds, ds_info = tfds.load(\"deep_weeds\", split=['train'], with_info=True, as_supervised=True)\n",
        "# deep_weeds is delivered as a single 'train' split; we'll create train/val/test ourselves\n",
        "dataset = ds[0]\n",
        "print(ds_info)\n",
        "CLASS_NAMES = ds_info.features['label'].names\n",
        "print(\"Classes:\", CLASS_NAMES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m569wuk5o1Mp",
        "outputId": "c106a9eb-a89b-42af-d581-23e05b393e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='deep_weeds',\n",
            "    full_name='deep_weeds/3.0.0',\n",
            "    description=\"\"\"\n",
            "    The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora.The selected weed species are local to pastoral grasslands across the state of Queensland.The images were collected from weed infestations at the following sites across Queensland: \"Black River\", \"Charters Towers\",  \"Cluden\", \"Douglas\", \"Hervey Range\", \"Kelso\", \"McKinlay\" and \"Paluma\".\n",
            "    \"\"\",\n",
            "    homepage='https://github.com/AlexOlsen/DeepWeeds',\n",
            "    data_dir='/root/tensorflow_datasets/deep_weeds/3.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=469.32 MiB,\n",
            "    dataset_size=469.99 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(256, 256, 3), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=9),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    nondeterministic_order=False,\n",
            "    splits={\n",
            "        'train': <SplitInfo num_examples=17509, num_shards=4>,\n",
            "    },\n",
            "    citation=\"\"\"@article{DeepWeeds2019,\n",
            "      author = {Alex Olsen and\n",
            "        Dmitry A. Konovalov and\n",
            "        Bronson Philippa and\n",
            "        Peter Ridd and\n",
            "        Jake C. Wood and\n",
            "        Jamie Johns and\n",
            "        Wesley Banks and\n",
            "        Benjamin Girgenti and\n",
            "        Owen Kenny and\n",
            "        James Whinney and\n",
            "        Brendan Calvert and\n",
            "        Mostafa {Rahimi Azghadi} and\n",
            "        Ronald D. White},\n",
            "      title = {{DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning}},\n",
            "      journal = {Scientific Reports},\n",
            "      year = 2019,\n",
            "      number = 2058,\n",
            "      month = 2,\n",
            "      volume = 9,\n",
            "      issue = 1,\n",
            "      day = 14,\n",
            "      url = \"https://doi.org/10.1038/s41598-018-38343-3\",\n",
            "      doi = \"10.1038/s41598-018-38343-3\"\n",
            "    }\"\"\",\n",
            ")\n",
            "Classes: ['Chinee apple', 'Lantana', 'Parkinsonia', 'Parthenium', 'Prickly acacia', 'Rubber vine', 'Siam weed', 'Snake weed', 'Negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: preprocessing + split\n",
        "IMG_SIZE = 224\n",
        "BATCH = 32\n",
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "dataset = dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# shuffle then split 80/10/10\n",
        "ds_size = 0\n",
        "for _ in dataset:\n",
        "    ds_size += 1\n",
        "print(\"Total images:\", ds_size)\n",
        "\n",
        "dataset = dataset.shuffle(20000, seed=SEED, reshuffle_each_iteration=False)\n",
        "train_size = int(0.8 * ds_size)\n",
        "val_size   = int(0.1 * ds_size)\n",
        "test_size  = ds_size - train_size - val_size\n",
        "\n",
        "train_ds = dataset.take(train_size).batch(BATCH).prefetch(AUTOTUNE)\n",
        "rest = dataset.skip(train_size)\n",
        "val_ds = rest.take(val_size).batch(BATCH).prefetch(AUTOTUNE)\n",
        "test_ds = rest.skip(val_size).batch(BATCH).prefetch(AUTOTUNE)\n",
        "\n",
        "print(\"Split sizes:\", train_size, val_size, test_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wol9Jki3o34y",
        "outputId": "ade95dd2-cd84-4157-ce0b-dcc22a29e73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 17509\n",
            "Split sizes: 14007 1750 1752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: model (transfer learning)\n",
        "base = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3),\n",
        "                                         include_top=False, weights='imagenet')\n",
        "base.trainable = False  # freeze base for speed\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "outputs = tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "JAYvkubRo5-m",
        "outputId": "7a6e6fb9-2adf-4792-9adf-f564c8624b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │        \u001b[38;5;34m11,529\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,529</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,269,513\u001b[0m (8.66 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,269,513</span> (8.66 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,529\u001b[0m (45.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,529</span> (45.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: train (use modest epochs now; enlarge if time allows)\n",
        "EPOCHS = 10   # start small to test; increase to 25-50 if time permits\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_deepweeds_mobilenet.h5\", save_best_only=True, monitor='val_sparse_categorical_accuracy', mode='max'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=6, restore_best_weights=True, mode='max', verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3HEMfRdo8xR",
        "outputId": "e929e36f-458a-435e-bd0f-1c44bc07580d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 68/438\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:41\u001b[0m 1s/step - loss: 2.3586 - sparse_categorical_accuracy: 0.1636"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: evaluate & plots\n",
        "# Evaluate on test set\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print(f\"Test acc: {acc:.4f}\")\n",
        "\n",
        "# Predictions for classification report / confusion matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for batch_x, batch_y in test_ds:\n",
        "    probs = model.predict(batch_x)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    y_true.extend(batch_y.numpy().tolist())\n",
        "    y_pred.extend(preds.tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# plot history\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history.history['sparse_categorical_accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'], label='val_acc')\n",
        "plt.legend(); plt.title('Accuracy'); plt.savefig('accuracy.png'); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend(); plt.title('Loss'); plt.savefig('loss.png'); plt.show()\n",
        "\n",
        "# plot confusion\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(cm, interpolation='nearest'); plt.title('Confusion matrix'); plt.colorbar()\n",
        "tick_marks = np.arange(len(CLASS_NAMES))\n",
        "plt.xticks(tick_marks, CLASS_NAMES, rotation=45, ha='right')\n",
        "plt.yticks(tick_marks, CLASS_NAMES)\n",
        "thresh = cm.max() / 2.\n",
        "for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i,j], 'd'), ha='center', color='white' if cm[i,j]>thresh else 'black')\n",
        "plt.tight_layout(); plt.savefig('confusion.png'); plt.show()\n",
        "\n",
        "# Save final model (if you want the TF SavedModel)\n",
        "model.save(\"deepweeds_mobilenet_savedmodel\")\n",
        "print(\"Saved model and plots.\")\n"
      ],
      "metadata": {
        "id": "TjGAEXd3o-z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}